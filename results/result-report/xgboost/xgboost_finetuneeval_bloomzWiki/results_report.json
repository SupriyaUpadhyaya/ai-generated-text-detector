{
    "Model Type": "xgboost",
    "Task": "finetuneAndEval",
    "Training Data": "bloomzWiki",
    "Data Type": "abstract",
    "New Line": "without",
    "Log Folder Name": "xgboost_finetuneeval_bloomzWiki",
    "Title": "XGBoost Finetune and Evaluation - bloomzWiki",
    "percentage": null,
    "train": true,
    "Train input path": "data/without_n_preprocessed/arxiv_bloomzWiki_train.jsonl",
    "Test input path": "data/without_n_preprocessed/arxiv_bloomzWiki_test.jsonl",
    "Validation input path": "data/without_n_preprocessed/arxiv_bloomzWiki_validation.jsonl",
    "Human text column name": "abstract",
    "Machine text column name": "machine_abstract",
    "log_path": "results/report/xgboost/xgboost_finetuneeval_bloomzWiki/",
    "Training accuracy": 0.83,
    "precision_score chatgpt_abstract_without": "0.5004170141784821",
    "F1 score chatgpt_abstract_without": "0.6670372429127293",
    "Accuracy Score chatgpt_abstract_without": "0.5008333333333334",
    "Recall Score chatgpt_abstract_without": "1.0",
    "precision_score chatgpt_abstract_paraphrased_without": "0.5004170141784821",
    "F1 score chatgpt_abstract_paraphrased_without": "0.6670372429127293",
    "Accuracy Score chatgpt_abstract_paraphrased_without": "0.5008333333333334",
    "Recall Score chatgpt_abstract_paraphrased_without": "1.0",
    "precision_score bloomz_abstract_without": "0.49958298582151794",
    "F1 score bloomz_abstract_without": "0.6659255141745414",
    "Accuracy Score bloomz_abstract_without": "0.49916666666666665",
    "Recall Score bloomz_abstract_without": "0.9983333333333333",
    "precision_score bloomz_abstract_paraphrased_without": "0.49958298582151794",
    "F1 score bloomz_abstract_paraphrased_without": "0.6659255141745414",
    "Accuracy Score bloomz_abstract_paraphrased_without": "0.49916666666666665",
    "Recall Score bloomz_abstract_paraphrased_without": "0.9983333333333333",
    "precision_score llama3_abstract_without": "0.4991652754590985",
    "F1 score llama3_abstract_without": "0.6651835372636262",
    "Accuracy Score llama3_abstract_without": "0.49833333333333335",
    "Recall Score llama3_abstract_without": "0.9966666666666667",
    "precision_score cohere_abstract_without": "0.5004170141784821",
    "F1 score cohere_abstract_without": "0.6670372429127293",
    "Accuracy Score cohere_abstract_without": "0.5008333333333334",
    "Recall Score cohere_abstract_without": "1.0",
    "precision_score davinci_abstract_without": "0.5004170141784821",
    "F1 score davinci_abstract_without": "0.6670372429127293",
    "Accuracy Score davinci_abstract_without": "0.5008333333333334",
    "Recall Score davinci_abstract_without": "1.0",
    "precision_score flant5_abstract_without": "0.4983277591973244",
    "F1 score flant5_abstract_without": "0.6636971046770601",
    "Accuracy Score flant5_abstract_without": "0.49666666666666665",
    "Recall Score flant5_abstract_without": "0.9933333333333333",
    "precision_score llama3_ml_without": "0.5",
    "F1 score llama3_ml_without": "0.6666666666666666",
    "Accuracy Score llama3_ml_without": "0.5",
    "Recall Score llama3_ml_without": "1.0",
    "precision_score bloomz_wiki_without": "0.5206093189964157",
    "F1 score bloomz_wiki_without": "0.6771561771561772",
    "Accuracy Score bloomz_wiki_without": "0.5383333333333333",
    "Recall Score bloomz_wiki_without": "0.9683333333333334",
    "precision_score chatgpt_wiki_without": "0.49023638232271327",
    "F1 score chatgpt_wiki_without": "0.6068702290076335",
    "Accuracy Score chatgpt_wiki_without": "0.48414023372287146",
    "Recall Score chatgpt_wiki_without": "0.7963272120200334",
    "precision_score cohere_wiki_without": "0.4520166898470097",
    "F1 score cohere_wiki_without": "0.5475989890480202",
    "Accuracy Score cohere_wiki_without": "0.42628205128205127",
    "Recall Score cohere_wiki_without": "0.6944444444444444",
    "precision_score davinci_wiki_without": "0.5141414141414141",
    "F1 score davinci_wiki_without": "0.640251572327044",
    "Accuracy Score davinci_wiki_without": "0.5233333333333333",
    "Recall Score davinci_wiki_without": "0.8483333333333334",
    "precision_score chatgptBloomz_abstract_without": "0.49979157982492706",
    "F1 score chatgptBloomz_abstract_without": "0.6662961933870519",
    "Accuracy Score chatgptBloomz_abstract_without": "0.4995833333333333",
    "Recall Score chatgptBloomz_abstract_without": "0.9991666666666666",
    "precision_score bloomzWiki_abstract_without": "0.8240589198036007",
    "F1 score bloomzWiki_abstract_without": "0.8315441783649876",
    "Accuracy Score bloomzWiki_abstract_without": "0.83",
    "Recall Score bloomzWiki_abstract_without": "0.8391666666666666",
    "precision_score bloomzWikiML_abstract_without": "0.6908157444381061",
    "F1 score bloomzWikiML_abstract_without": "0.6816774556712637",
    "Accuracy Score bloomzWikiML_abstract_without": "0.6858333333333333",
    "Recall Score bloomzWikiML_abstract_without": "0.6727777777777778"
}