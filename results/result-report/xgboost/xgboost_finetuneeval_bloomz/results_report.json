{
    "Model Type": "xgboost",
    "Task": "finetuneAndEval",
    "Training Data": "bloomz",
    "Data Type": "abstract",
    "New Line": "without",
    "Log Folder Name": "xgboost_finetuneeval_bloomz",
    "Title": "XGBoost Finetune and Evaluation - bloomz",
    "percentage": null,
    "train": true,
    "Train input path": "data/without_n_preprocessed/arxiv_bloomz_train.jsonl",
    "Test input path": "data/without_n_preprocessed/arxiv_bloomz_test.jsonl",
    "Validation input path": "data/without_n_preprocessed/arxiv_bloomz_val.jsonl",
    "Human text column name": "abstract",
    "Machine text column name": "machine_abstract",
    "log_path": "results/report/xgboost/xgboost_finetuneeval_bloomz/",
    "Training accuracy": 0.7675,
    "precision_score chatgpt_abstract_without": "0.2191780821917808",
    "F1 score chatgpt_abstract_without": "0.11721611721611722",
    "Accuracy Score chatgpt_abstract_without": "0.3975",
    "Recall Score chatgpt_abstract_without": "0.08",
    "precision_score chatgpt_abstract_paraphrased_without": "0.2191780821917808",
    "F1 score chatgpt_abstract_paraphrased_without": "0.11721611721611722",
    "Accuracy Score chatgpt_abstract_paraphrased_without": "0.3975",
    "Recall Score chatgpt_abstract_paraphrased_without": "0.08",
    "precision_score bloomz_abstract_without": "0.7609756097560976",
    "F1 score bloomz_abstract_without": "0.7703703703703704",
    "Accuracy Score bloomz_abstract_without": "0.7675",
    "Recall Score bloomz_abstract_without": "0.78",
    "precision_score bloomz_abstract_paraphrased_without": "0.7609756097560976",
    "F1 score bloomz_abstract_paraphrased_without": "0.7703703703703704",
    "Accuracy Score bloomz_abstract_paraphrased_without": "0.7675",
    "Recall Score bloomz_abstract_paraphrased_without": "0.78",
    "precision_score llama3_abstract_without": "0.023391812865497075",
    "F1 score llama3_abstract_without": "0.010376134889753566",
    "Accuracy Score llama3_abstract_without": "0.3641666666666667",
    "Recall Score llama3_abstract_without": "0.006666666666666667",
    "precision_score cohere_abstract_without": "0.36",
    "F1 score cohere_abstract_without": "0.22628571428571428",
    "Accuracy Score cohere_abstract_without": "0.43583333333333335",
    "Recall Score cohere_abstract_without": "0.165",
    "precision_score davinci_abstract_without": "0.536",
    "F1 score davinci_abstract_without": "0.4123076923076923",
    "Accuracy Score davinci_abstract_without": "0.5225",
    "Recall Score davinci_abstract_without": "0.335",
    "precision_score flant5_abstract_without": "0.32936507936507936",
    "F1 score flant5_abstract_without": "0.19483568075117372",
    "Accuracy Score flant5_abstract_without": "0.42833333333333334",
    "Recall Score flant5_abstract_without": "0.13833333333333334",
    "precision_score llama3_ml_without": "0.4149797570850202",
    "F1 score llama3_ml_without": "0.37477148080438755",
    "Accuracy Score llama3_ml_without": "0.43",
    "Recall Score llama3_ml_without": "0.3416666666666667",
    "precision_score bloomz_wiki_without": "0.5828144458281445",
    "F1 score bloomz_wiki_without": "0.6671418389166073",
    "Accuracy Score bloomz_wiki_without": "0.6108333333333333",
    "Recall Score bloomz_wiki_without": "0.78",
    "precision_score chatgpt_wiki_without": "0.35845213849287166",
    "F1 score chatgpt_wiki_without": "0.3229357798165138",
    "Accuracy Score chatgpt_wiki_without": "0.38397328881469117",
    "Recall Score chatgpt_wiki_without": "0.2938230383973289",
    "precision_score cohere_wiki_without": "0.44251626898047725",
    "F1 score cohere_wiki_without": "0.43918191603875134",
    "Accuracy Score cohere_wiki_without": "0.44337606837606836",
    "Recall Score cohere_wiki_without": "0.4358974358974359",
    "precision_score davinci_wiki_without": "0.44582593250444047",
    "F1 score davinci_wiki_without": "0.4316423043852107",
    "Accuracy Score davinci_wiki_without": "0.44916666666666666",
    "Recall Score davinci_wiki_without": "0.41833333333333333",
    "precision_score chatgptBloomz_abstract_without": "0.6206451612903225",
    "F1 score chatgptBloomz_abstract_without": "0.4870886075949367",
    "Accuracy Score chatgptBloomz_abstract_without": "0.5779166666666666",
    "Recall Score chatgptBloomz_abstract_without": "0.4008333333333333",
    "precision_score bloomzWiki_abstract_without": "0.6161548731642189",
    "F1 score bloomzWiki_abstract_without": "0.6842105263157895",
    "Accuracy Score bloomzWiki_abstract_without": "0.645",
    "Recall Score bloomzWiki_abstract_without": "0.7691666666666667",
    "precision_score bloomzWikiML_abstract_without": "0.5922381711855396",
    "F1 score bloomzWikiML_abstract_without": "0.6052703069817984",
    "Accuracy Score bloomzWikiML_abstract_without": "0.5963888888888889",
    "Recall Score bloomzWikiML_abstract_without": "0.6188888888888889"
}