{
    "Model Type": "xgboost",
    "Task": "finetuneAndEval",
    "Training Data": "bloomz",
    "Data Type": "abstract",
    "New Line": "with",
    "Log Folder Name": "xgboost_finetuneeval_with_bloomz",
    "Title": "XGBoost Finetune and Evaluation - with - bloomz",
    "percentage": null,
    "train": true,
    "Train input path": "data/with_n/arxiv_bloomz_train.jsonl",
    "Test input path": "data/with_n/arxiv_bloomz_test.jsonl",
    "Validation input path": "data/with_n/arxiv_bloomz_validation.jsonl",
    "Human text column name": "abstract",
    "Machine text column name": "machine_abstract",
    "log_path": "results/report/xgboost/xgboost_finetuneeval_with_bloomz/",
    "Training accuracy": 0.795,
    "precision_score chatgpt_abstract_with": "0.4794188861985472",
    "F1 score chatgpt_abstract_with": "0.39091806515301086",
    "Accuracy Score chatgpt_abstract_with": "0.48583333333333334",
    "Recall Score chatgpt_abstract_with": "0.33",
    "precision_score bloomz_abstract_with": "0.8020477815699659",
    "F1 score bloomz_abstract_with": "0.7925801011804384",
    "Accuracy Score bloomz_abstract_with": "0.795",
    "Recall Score bloomz_abstract_with": "0.7833333333333333",
    "precision_score chatgpt_abstract_paraphrased_with": "0.4794188861985472",
    "F1 score chatgpt_abstract_paraphrased_with": "0.39091806515301086",
    "Accuracy Score chatgpt_abstract_paraphrased_with": "0.48583333333333334",
    "Recall Score chatgpt_abstract_paraphrased_with": "0.33",
    "precision_score bloomz_abstract_paraphrased_with": "0.8020477815699659",
    "F1 score bloomz_abstract_paraphrased_with": "0.7925801011804384",
    "Accuracy Score bloomz_abstract_paraphrased_with": "0.795",
    "Recall Score bloomz_abstract_paraphrased_with": "0.7833333333333333",
    "precision_score cohere_abstract_with": "0.6853448275862069",
    "F1 score cohere_abstract_with": "0.38221153846153844",
    "Accuracy Score cohere_abstract_with": "0.5716666666666667",
    "Recall Score cohere_abstract_with": "0.265",
    "precision_score davinci_abstract_with": "0.554016620498615",
    "F1 score davinci_abstract_with": "0.4162330905306972",
    "Accuracy Score davinci_abstract_with": "0.5325",
    "Recall Score davinci_abstract_with": "0.3333333333333333",
    "precision_score flant5_abstract_with": "0.44193548387096776",
    "F1 score flant5_abstract_with": "0.3010989010989011",
    "Accuracy Score flant5_abstract_with": "0.47",
    "Recall Score flant5_abstract_with": "0.22833333333333333",
    "precision_score llama3_ml_with": "0.6889632107023411",
    "F1 score llama3_ml_with": "0.4582869855394883",
    "Accuracy Score llama3_ml_with": "0.5941666666666666",
    "Recall Score llama3_ml_with": "0.3433333333333333",
    "precision_score bloomz_wiki_with": "0.49409780775716694",
    "F1 score bloomz_wiki_with": "0.6562150055991042",
    "Accuracy Score bloomz_wiki_with": "0.48833333333333334",
    "Recall Score bloomz_wiki_with": "0.9766666666666667",
    "precision_score chatgpt_wiki_with": "0.47822299651567945",
    "F1 score chatgpt_wiki_with": "0.6285060103033773",
    "Accuracy Score chatgpt_wiki_with": "0.4582637729549249",
    "Recall Score chatgpt_wiki_with": "0.9165275459098498",
    "precision_score cohere_wiki_with": "0.48624862486248627",
    "F1 score cohere_wiki_with": "0.6419753086419753",
    "Accuracy Score cohere_wiki_with": "0.4732905982905983",
    "Recall Score cohere_wiki_with": "0.9444444444444444",
    "precision_score davinci_wiki_with": "0.48186528497409326",
    "F1 score davinci_wiki_with": "0.6348122866894198",
    "Accuracy Score davinci_wiki_with": "0.465",
    "Recall Score davinci_wiki_with": "0.93"
}