{
    "Model Type": "xgboost",
    "Task": "finetuneAndEval",
    "Training Data": "bloomzWikiML",
    "Data Type": "abstract",
    "New Line": "without",
    "Log Folder Name": "xgboost_finetuneeval_bloomzWikiML",
    "Title": "XGBoost Finetune and Evaluation - bloomzWikiML",
    "percentage": null,
    "train": true,
    "Train input path": "data/without_n_preprocessed/arxiv_bloomzWikiML_train.jsonl",
    "Test input path": "data/without_n_preprocessed/arxiv_bloomzWikiML_test.jsonl",
    "Validation input path": "data/without_n_preprocessed/arxiv_bloomzWikiML_validation.jsonl",
    "Human text column name": "abstract",
    "Machine text column name": "machine_abstract",
    "log_path": "results/report/xgboost/xgboost_finetuneeval_bloomzWikiML/",
    "Training accuracy": 0.7286111111111111,
    "precision_score chatgpt_abstract_without": "0.5004170141784821",
    "F1 score chatgpt_abstract_without": "0.6670372429127293",
    "Accuracy Score chatgpt_abstract_without": "0.5008333333333334",
    "Recall Score chatgpt_abstract_without": "1.0",
    "precision_score chatgpt_abstract_paraphrased_without": "0.5004170141784821",
    "F1 score chatgpt_abstract_paraphrased_without": "0.6670372429127293",
    "Accuracy Score chatgpt_abstract_paraphrased_without": "0.5008333333333334",
    "Recall Score chatgpt_abstract_paraphrased_without": "1.0",
    "precision_score bloomz_abstract_without": "0.49958298582151794",
    "F1 score bloomz_abstract_without": "0.6659255141745414",
    "Accuracy Score bloomz_abstract_without": "0.49916666666666665",
    "Recall Score bloomz_abstract_without": "0.9983333333333333",
    "precision_score bloomz_abstract_paraphrased_without": "0.49958298582151794",
    "F1 score bloomz_abstract_paraphrased_without": "0.6659255141745414",
    "Accuracy Score bloomz_abstract_paraphrased_without": "0.49916666666666665",
    "Recall Score bloomz_abstract_paraphrased_without": "0.9983333333333333",
    "precision_score llama3_abstract_without": "0.49958298582151794",
    "F1 score llama3_abstract_without": "0.6659255141745414",
    "Accuracy Score llama3_abstract_without": "0.49916666666666665",
    "Recall Score llama3_abstract_without": "0.9983333333333333",
    "precision_score cohere_abstract_without": "0.5004170141784821",
    "F1 score cohere_abstract_without": "0.6670372429127293",
    "Accuracy Score cohere_abstract_without": "0.5008333333333334",
    "Recall Score cohere_abstract_without": "1.0",
    "precision_score davinci_abstract_without": "0.5004170141784821",
    "F1 score davinci_abstract_without": "0.6670372429127293",
    "Accuracy Score davinci_abstract_without": "0.5008333333333334",
    "Recall Score davinci_abstract_without": "1.0",
    "precision_score flant5_abstract_without": "0.4991652754590985",
    "F1 score flant5_abstract_without": "0.6651835372636262",
    "Accuracy Score flant5_abstract_without": "0.49833333333333335",
    "Recall Score flant5_abstract_without": "0.9966666666666667",
    "precision_score llama3_ml_without": "0.5004170141784821",
    "F1 score llama3_ml_without": "0.6670372429127293",
    "Accuracy Score llama3_ml_without": "0.5008333333333334",
    "Recall Score llama3_ml_without": "1.0",
    "precision_score bloomz_wiki_without": "0.5292529252925292",
    "F1 score bloomz_wiki_without": "0.6873173582700175",
    "Accuracy Score bloomz_wiki_without": "0.5541666666666667",
    "Recall Score bloomz_wiki_without": "0.98",
    "precision_score chatgpt_wiki_without": "0.5403669724770642",
    "F1 score chatgpt_wiki_without": "0.6974541148608644",
    "Accuracy Score chatgpt_wiki_without": "0.5734557595993323",
    "Recall Score chatgpt_wiki_without": "0.9833055091819699",
    "precision_score cohere_wiki_without": "0.5186074429771909",
    "F1 score cohere_wiki_without": "0.6641045349730976",
    "Accuracy Score cohere_wiki_without": "0.5331196581196581",
    "Recall Score cohere_wiki_without": "0.9230769230769231",
    "precision_score davinci_wiki_without": "0.5394862036156042",
    "F1 score davinci_wiki_without": "0.6868564506359782",
    "Accuracy Score davinci_wiki_without": "0.5691666666666667",
    "Recall Score davinci_wiki_without": "0.945",
    "precision_score chatgptBloomz_abstract_without": "0.49979157982492706",
    "F1 score chatgptBloomz_abstract_without": "0.6662961933870519",
    "Accuracy Score chatgptBloomz_abstract_without": "0.4995833333333333",
    "Recall Score chatgptBloomz_abstract_without": "0.9991666666666666",
    "precision_score bloomzWiki_abstract_without": "0.7998405103668261",
    "F1 score bloomzWiki_abstract_without": "0.8174409127954361",
    "Accuracy Score bloomzWiki_abstract_without": "0.8133333333333334",
    "Recall Score bloomzWiki_abstract_without": "0.8358333333333333",
    "precision_score bloomzWikiML_abstract_without": "0.7320924985899605",
    "F1 score bloomzWikiML_abstract_without": "0.7265603134620767",
    "Accuracy Score bloomzWikiML_abstract_without": "0.7286111111111111",
    "Recall Score bloomzWikiML_abstract_without": "0.7211111111111111"
}